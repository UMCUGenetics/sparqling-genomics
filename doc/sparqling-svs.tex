\documentclass[11pt,a4paper]{book}
\usepackage{umcu}
\title{SPARQLing structural variation}
\author{Roel Janssen}
%\date{January August 2017 --- September 2017}

\begin{document}

\begin{titlepage}
  \maketitle
  \thispagestyle{empty}
\end{titlepage}


\setcounter{page}{1}
\pagenumbering{roman}
\hypersetup{linkcolor=black}
\tableofcontents
\newpage{}

\setcounter{page}{1}
\pagenumbering{arabic}

\chapter{Introduction}


\section{Prerequisites}
\label{sec:prerequisites}

  The tools provided by this project do not include a triple store.  There are
  great triple stores out there like \href{https://virtuoso.openlinksw.com/}
  {Virtuoso}, \href{https://github.com/4store/4store}{4store}, and
  \href{https://www.blazegraph.com/}{BlazeGraph}.

  Before we can use the programs provided by this project, we need to build
  them first.

  The build system needs \href{https://www.gnu.org/software/autoconf}{GNU Autoconf},
  \href{https://www.gnu.org/software/automake}{GNU Automake},
  \href{https://www.gnu.org/software/make}{GNU Make} and
  \href{https://www.freedesktop.org/wiki/Software/pkg-config/}{pkg-config}.
  Additionally, for building the documentation, a working \LaTeX{} distribution is
  required including the \texttt{pdflatex} program.  Because \LaTeX{} distributions
  are rather large, this is optional.

  Each component in the repository has its own dependencies.  Table
  \ref{table:dependencies} provides an overview for each tool.

  \hypersetup{urlcolor=black}
  \begin{table}[H]
    \begin{tabularx}{\textwidth}{ X X X }
      \headrow
      \textbf{vcf2turtle} & \textbf{turtle2remote} & \textbf{Web interface}\\
      \evenrow
      \href{https://gcc.gnu.org/}{GNU C compiler}
      & \href{https://gcc.gnu.org/}{GNU C compiler}
      & \href{https://www.gnu.org/software/guile}{GNU Guile}\\
      \oddrow
      \href{https://www.gnupg.org/related_software/libgcrypt/}{libgcrypt}
      & \href{https://curl.haxx.se/}{cURL}
      & \href{https://github.com/OrangeShark/guile-commonmark}{Guile-Commonmark}\\
      \evenrow
      \href{http://www.htslib.org/}{HTSLib}
      &
      & \href{http://savannah.nongnu.org/projects/guile-json/}{Guile-JSON}\\
    \end{tabularx}
    \caption{\small External tools required to build and run the programs this
      project provides.}
    \label{table:dependencies}
  \end{table}
  \hypersetup{urlcolor=LinkGray}

\section{Setting up a build environment with GNU Guix}

  If \href{https://www.gnu.org/software/guix}{GNU Guix} is available on your
  system, An environment that contains all external tools required to build
  the programs in this project can be obtained running the following command
  from the project's repository root:

\begin{siderules}
\begin{verbatim}
guix environment -l guix.scm
\end{verbatim}
\end{siderules}

\section{Installation instructions}

After installing the required tools (see section \ref{sec:prerequisites}), the
entire project can be built using the Autotools build system commands:
\begin{siderules}
\begin{verbatim}
autoreconf -vfi
./configure
make
make install
\end{verbatim}
\end{siderules}

\chapter{Command-line programs}

  The project provides programs to create a complete pipeline including
  data conversion, data importing and data exploration.  The tasks we can
  perform with the command-line programs are:
  \begin{itemize}
    \item Extract triples from VCF files;
    \item Push data to a SPARQL endpoint.
  \end{itemize}

\section{Preparing data with \texttt{vcf2turtle}}

  Obtaining variants from sequenced data is a task of so called
  \emph{variant callers}.  These programs output the variants they found in
  the \emph{Variant Call Format} (VCF).  Before we can use the data described
  in this format, we need to extract triples from it.

  The \texttt{vcf2turtle} program does exactly this, by converting a VCF file
  into the \emph{Terse RDF Triple Language} (Turtle) format.  In section
  \ref{sec:turtle2remote} \textcolor{LinkGray}{`}\nameref{sec:turtle2remote}%
  \textcolor{LinkGray}{'} we describe how to import the data produced by
  \texttt{vcf2turtle} in the database.

\subsection{Data extracted by \texttt{vcf2turtle}}

  The triples \texttt{vcf2turtle} extracts are:
  \begin{itemize}
    \item Chromosome and position from the \texttt{CHROM} and \texttt{POS}
      columns;
    \item Filter names from the \texttt{FILTER} column;
    \item Quality from the \texttt{QUAL} column;
  \end{itemize}

  For structural variants, the following extra triples are extracted:
  \begin{itemize}
    \item Chromosome and position of the second location specified in the
      \texttt{CHR2} and \texttt{END} fields.
    \item Confidence interval using the \texttt{CIPOS} and \texttt{CIEND}
      fields.
    \item Structural variant type specified in the \texttt{SVTYPE} field.
  \end{itemize}

\subsection{Example usage}

\begin{verbatim}
$ vcf2turtle --input-file=/path/to/my/variants.vcf > /path/to/my/variants.ttl
\end{verbatim}

\subsection{Run-time properties}

  The program typically uses less than one megabyte of memory, because it
  processes the variant calls one-by-one.  It reads through the file from top
  to bottom, so its running time is dependent on the input file size.  The
  throughput of this program is around $20$ to $60$ megabytes per second,
  depending on your computer.

\section{Importing data with \texttt{turtle2remote}}
\label{sec:turtle2remote}

  To import data in the \emph{Terse RDF Triple Language} (Turtle) format in
  a triple store (our database), we can use the \texttt{turtle2remote} program.

  The triple stores typically store data in \emph{graphs}.  One triple store
  can host multiple graphs, so we must tell the triple store which graph we
  would like to add the data to.

\subsection{Example usage}

\begin{verbatim}
$ turtle2remote --input-file=/path/to/my/variants.ttl        \
                --remote-url=http://localhost:8890           \
                --graph-iri=http://localhost:8890/MyVariants
\end{verbatim}

\chapter{Web interface}

  In addition to the command-line programs, the project provides a web
  interface for prototyping queries, and quick data reporting.  With the
  web interface you can:
  \begin{itemize}
  \item Write and execute your own SPARQL queries;
  \item Import and export data;
  \item Get a general idea about what your data looks like.
  \end{itemize}

\chapter{Programming in Python, Perl, R, Scheme, C, and C++}

\end{document}