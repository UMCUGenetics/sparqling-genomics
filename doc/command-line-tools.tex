\chapter{Command-line programs}
\label{chap:command-line}

  SPARQLing genomics provides programs to create an extensive knowledge graph
  from genomics-specific data formats.  The programs described in this chapter
  provide the ``layer 0'' for the knowledge graph, and the tools to discover
  the data in this layer.   All tools described in the remainder of this
  chapter can be invoked with the \texttt{-{}-help} argument to get a complete
  overview of options for that particular tool.

\section{Preparing variant call data with \texttt{vcf2rdf}}
\label{sec:vcf2rdf}

  Obtaining variants from sequenced data is a task performed by \emph{variant
    callers}.  These programs often output the variants they found in the
  \emph{Variant Call Format} (VCF).  The \texttt{vcf2rdf} program extracts
  knowledge from a VCF file and writes it as RDF.

\subsection{Knowledge extracted by \texttt{vcf2rdf}}

  The program treats the VCF as its own ontology.  It uses the VCF header as
  a guide.  All fields described in the header of the VCF file will be
  translated into triples.  In addition to the knowledge from the VCF file,
  \texttt{vcf2rdf} provides the following triples:

  \begin{table}[H]
    \begin{tabularx}{\textwidth}{*{3}{!{\VRule[-1pt]}l}!{\VRule[-1pt]}L}
      \headrow
      \textbf{Subject} & \textbf{Predicate} & \textbf{Object}
      & \textbf{Description}\\
      \evenrow
      \texttt{:Origin/}\emph{identifier} & \texttt{rdf:type} & \texttt{:Origin}
      & This defines a uniquely identifiable reference to the originating file.\\
      \oddrow
      \texttt{:Origin/}\emph{identifier} & \texttt{:filename} &
      \emph{filename}
      & This triple states the originating filename.\\
      \evenrow
      \texttt{:Origin/}\emph{identifier} & \texttt{:sha256sum} &
      \emph{SHA256 sum}
      & This triple states the SHA256 sum of the content of the original file.\\
      \oddrow
      \texttt{:Sample/}\emph{sample name} & \texttt{rdf:type} & \texttt{:Sample}
      & This states that there is a sample with \emph{sample name}.\\
      \evenrow
      \texttt{:Sample/}\emph{sample name} & \texttt{:foundIn}
      & \texttt{:Origin/}\emph{identifier}
      & This triple states that a sample can be found in a file identified by
      the \texttt{:Origin} with a specific identifier.\\
      \oddrow
      \texttt{:Origin/}\emph{identifier} & \texttt{:convertedBy} &
             {\fontfamily{\ttdefault}\selectfont :vcf2rdf-\sgversion{}}
      & This triple states that the file was converted with \texttt{vcf2rdf}.\\
    \end{tabularx}
    \caption{\small The additional triple patterns provided by \texttt{vcf2rdf}.}
    \label{table:vcf2rdf-ontology}
  \end{table}

  The following snippet is an example of the extra data in Turtle-format:

\begin{siderules}
\begin{lstlisting}
<http://sparqling-genomics/Origin/14f2b609b>
    :convertedBy :vcf2rdf-(@*\sgversion{}*@) ;
    :filename "clone_ref_tumor.vcf.gz"^^xsd:string ;
    :sha256sum "14f2b609b" ;
    a :Origin .

<http://sparqling-genomics/Sample/CLONE_REF>
    :foundIn <http://sparqling-genomics/Origin/14f2b609b3> ;
    a :Sample .

<http://sparqling-genomics/Sample/CLONE_TUMOR>
    :foundIn <http://sparqling-genomics/Origin/14f2b609b3> ;
    a :Sample .
\end{lstlisting}
\end{siderules}

\subsection{Handling multi-sample VCF files}

  From version \texttt{0.99.11} onwards, \program{vcf2rdf} only writes variants
  that have at least one alternative allele specified.  It will reserve the
  numeric ID to preserve compatibility with previous and future versions.

  The numeric IDs are calculated left-to-right, top-to-bottom.  So for a file
  containing two samples, the first variant for the first sample will receive
  the numeric ID $0$, and the first variant for the second sample will receive
  numeric ID $1$.  The second variant for the first sample receives numeric ID
  $2$, and so on.

\subsection{Example usage}

\begin{siderules}
\begin{verbatim}
vcf2rdf -i /path/to/my/variants.vcf > /path/to/my/variants.ttl
\end{verbatim}
\end{siderules}

To get a complete overview of options for this program, use:
\begin{siderules}
\begin{verbatim}
vcf2rdf --help
\end{verbatim}
\end{siderules}

\subsection{Run-time properties}

  Depending on the serialization format, the program typically uses from two megabytes
  (in \texttt{ntriples} mode), to multiple times the size of the input VCF
  (in \texttt{turtle} mode).

  The \texttt{ntriples} mode can output triples as soon as they are formed, while the
  \texttt{turtle} mode waits until all triples are known, so that it can output them
  efficiently, producing compact output at the cost of using more memory.

  We recommend using the \texttt{ntriples} format for large input files, and
  \texttt{turtle} for small input files.  The following example illustrates how to
  use \texttt{ntriples} mode.

\begin{siderules}
\begin{verbatim}
vcf2rdf -i /path/to/my/variants.vcf -O ntriples > /path/to/my/variants.n3
\end{verbatim}
\end{siderules}

\section{Preparing sequence alignment maps with \texttt{bam2rdf}}

  Aligning reads from a DNA sequencer to a predetermined \emph{reference genome}
  is a task performed by \emph{read mapper} programs.  Oftentimes, the output
  produced by these programs are in the \emph{sequence alignment map} (SAM) format,
  or its equivalent \emph{binary alignment map} (BAM) format.  The \texttt{bam2rdf}
  program can read data in either format.

\subsection{Knowledge extracted by \texttt{bam2rdf}}

  The current version of \texttt{bam2rdf} merely extracts information from the
  alignment map header.

  \begin{table}[H]
    \begin{tabularx}{\linewidth}{>{\hsize=0.2\hsize}X
        !{\VRule[-1pt]}>{\hsize=0.15\hsize}X
        !{\VRule[-1pt]}>{\hsize=0.35\hsize}L
        !{\VRule[-1pt]}>{\hsize=0.30\hsize}L}
      %\begin{tabularx}{\textwidth}{L l L L }
      \headrow
      \textbf{Subject} & \textbf{Predicate} & \textbf{Object}
      & \textbf{Description}\\
      \evenrow
      \texttt{:Origin/}\emph{identifier} & \texttt{rdf:type} & \texttt{:Origin}
      & This defines a uniquely identifiable reference to the originating file.\\
      \oddrow
      \texttt{:Origin/}\emph{identifier} & \texttt{:filename} &
      \emph{filename}
      & This triple states the originating filename.\\
      \evenrow
      \texttt{:bam2rdf/}\emph{unique identifier}
      & \texttt{rdf:type} & One of: \texttt{:bam2rdf/HeaderItem},
      \texttt{:bam2rdf/ReferenceSequence}, \texttt{:bam2rdf/ReadGroup},
      \texttt{:bam2rdf/Program}, \texttt{:bam2rdf/Comment}.
      & The \emph{objects} correspond to the various types of header lines that
      can occur in the SAM format.\\
      \oddrow
      \texttt{:bam2rdf/}\emph{unique identifier}
      & \texttt{:foundIn}
      & \texttt{:Origin/}\emph{identifier}
      & This triple states that a header line can be found in a file identified
      by the \texttt{:Origin} with a specific identifier.\\
      \evenrow
      \texttt{:bam2rdf/}\emph{unique identifier}
      & \emph{type class}\texttt{/}\emph{key}
      & Literal value.
      & Each header field consists of a key/value pair.  The key is used as
      predicate.\\
      \oddrow
      \texttt{:Origin/}\emph{identifier}
      & \texttt{:convertedBy}
      & \texttt{:bam2rdf}
      & This triple states that the file was converted with \texttt{bam2rdf}.\\
    \end{tabularx}
    \caption{\small The additional triple patterns provided by \texttt{bam2rdf}.}
    \label{table:bam2rdf-ontology}
  \end{table}

\section{Preparing tabular data with \texttt{table2rdf}}
\label{sec:table2rdf}

  Data that can be represented as a table, like comma-separated values (CSV)
  or BED files, can be imported using \texttt{table2rdf}.  The column headers
  are used as predicates, and each row gets a unique row ID.  Non-alphanumeric
  characters in the header line are replaced by underscores, and all characters
  are replaced by their lowercase equivalent to make a consistent scheme for
  predicates.

  When the file does not contain a header line, one can be specified using the
  \texttt{-{}-header-line} argument.  When using this command-line argument, the
  delimiter must be a semicolon (\texttt{;}).

  The program can also read files compressed with \texttt{gzip}.

\subsection{Transforming objects}

  Unfortunately, \texttt{table2rdf} knows nothing about ontologies.  So when
  the input table has a column ``Chromosome'', by default \texttt{table2rdf}
  will treat these cells as literal values (as a \texttt{string}).  A
  \emph{transformer} can be used to express a column as an \emph{individual} in
  RDF.  An example might explain this best.

  Take the following input file:
\begin{siderules}
\begin{verbatim}
$ cat test.tsv
Chromosome      Position
chr1    1500000
chrMT   11000
\end{verbatim}
\end{siderules}

  Running \texttt{table2rdf} with its default settings will produce:

\begin{siderules}
\begin{verbatim}
$ table2rdf -i test.tsv
...
<http://sparqling-genomics/table2rdf/Row/...-R0000000000>
    sg:originatedFrom <http://sparqling-genomics/...> ;
    col:chromosome "chr1"^^xsd:string ;
    col:position 1500000 ;
    a :Row .

<http://sparqling-genomics/table2rdf/Row/...-R0000000001>
    sg:originatedFrom <http://sparqling-genomics/...> ;
    col:chromosome "chrMT"^^xsd:string ;
    col:position 11000 ;
    a :Row .
...
\end{verbatim}
\end{siderules}

  \begin{sloppypar}
  When we know that the data in a column refers to items in an ontology, like
  chromosomes defined in
  \href{http://rdf.biosemantics.org/data/genomeassemblies/hg19}%
  {<http://rdf.biosemantics.org/data/genomeassemblies/hg19>}, \texttt{table2rdf}
  can be told to use that ontology to describe that column.
  \end{sloppypar}

  To do so, we use the \texttt{-{}-transform-object} option, or \texttt{-t}
  for short:

\begin{siderules}
\begin{lstlisting}
$ table2rdf     \
    -i test.tsv \
    -t Chromosome=http://rdf.biosemantics.org/data/genomeassemblies/hg19#
...
(@*\colorbox{Highlight}{@prefix p00000: <http://rdf.biosemantics.org/data/genomeassemblies/hg19\#> .}*@)
...
<http://sparqling-genomics/table2rdf/Row/...-R0000000000>
    sg:originatedFrom <http://sparqling-genomics/...> ;
    (@*\colorbox{Highlight}{col:chromosome p00000:chr1 ;}*@)
    col:position 1500000 ;
    a :Row .

<http://sparqling-genomics/table2rdf/Row/...-R0000000001>
    sg:originatedFrom <http://sparqling-genomics/...> ;
    (@*\colorbox{Highlight}{col:chromosome p00000:chrMT ;}*@)
    col:position 11000 ;
    a :Row .
...
\end{lstlisting}
\end{siderules}

  After the transformation, the output produced by \texttt{table2rdf} uses
  URIs pointing to the ontology instead of literal values for chromosomes.

\subsection{Transforming predicates}

  Like transforming a cell in a table to a URI instead of a literal value,
  we can also specify the value for the column name.  By default, the column
  names are transformed using the \texttt{:table2rdf/Column/} prefix (e.g.
  \texttt{chromosome} becomes
  \texttt{http://sparqling-genomics/table2rdf/Column/chromosome}).  By using
  the \texttt{-{}-transform-predicate} option, or \texttt{-T} for short, a
  different transformation can be made:

\begin{siderules}
\begin{lstlisting}
$ table2rdf     \
    -i test.tsv \
    -t Chromosome=http://rdf.biosemantics.org/data/genomeassemblies/hg19#
    -T Chromosome=http://biohackathon.org/resource/faldo#reference
...
(@*\colorbox{Highlight}{@prefix p00000: <http://rdf.biosemantics.org/data/genomeassemblies/hg19\#> .}*@)
(@*\colorbox{Highlight}{@prefix p00001: <http://biohackathon.org/resource/faldo\#> .}*@)

<http://sparqling-genomics/table2rdf/Row/...-R0000000000>
    sg:originatedFrom <http://sparqling-genomics/...> ;
    (@*\colorbox{Highlight}{p00001:reference p00000:chr1 ;}*@)
    col:position 1500000 ;
    a :Row .

<http://sparqling-genomics/table2rdf/Row/...-R0000000001>
    sg:originatedFrom <http://sparqling-genomics/...> ;
    (@*\colorbox{Highlight}{p00001:reference p00000:chrMT ;}*@)
    col:position 11000 ;
    a :Row .
...
\end{lstlisting}
\end{siderules}

\subsection{Delimiters}

  Tabular data consists of rows and columns.  A field is a specific place in
  a table, having a column-coordinate, and a row-coordinate.  To distinguish
  fields from one another we use a delimiter.  Which delimiter to use (a tab,
  a comma, or a semicolon, etc.) is up to the dataset.  The delimiter
  can be chosen using the \texttt{-{}-delimiter} option, or \texttt{-d} for
  short.

  Sometimes a single field can consist of multiple ``subfields''.  To
  distinguish subfields, we use a secondary delimiter.  In RDF, we can split
  those subfields by using the same predicate as we would use for the entire
  field.  Using the \texttt{-{}-secondary-delimiter} option, we can invoke
  this behavior.

  The following example demonstrates the usage of \texttt{-{}-delimiter} and
  \texttt{-{}-secondary-delimiter}.  Take the following input file:
\begin{siderules}
\begin{verbatim}
$ cat multi.tsv
Chromosome	Position	Filter
1	10000	A;B;C;D
\end{verbatim}
\end{siderules}

  Without using the secondary delimiter, we get:

\begin{siderules}
\begin{lstlisting}
$ table2rdf -i multi.tsv
...
<http://sparqling-genomics/table2rdf/Row/...-R0000000000>
    sg:originatedFrom <http://sparqling-genomics/...> ;
    col:chromosome 1 ;
    (@*\colorbox{Highlight}{col:filter "A;B;C;D"\^{}\^{}xsd:string ;}*@)
    col:position 10000 ;
    a :Row .
\end{lstlisting}
\end{siderules}

  Using the secondary delimiter, we get:

\begin{siderules}
\begin{lstlisting}
$ table2rdf -i multi.tsv --secondary-delimiter ";"
...
<http://sparqling-genomics/table2rdf/Row/...-R0000000000>
    sg:originatedFrom <http://sparqling-genomics/...> ;
    col:chromosome 1 ;
    (@*\colorbox{Highlight}{col:filter "A"\^{}\^{}xsd:string, "B"\^{}\^{}xsd:string, "C"\^{}\^{}xsd:string,}*@)
    (@*\colorbox{Highlight}{\space\space\space\space\space\space\space\space\space\space\space"D"\^{}\^{}xsd:string ;}*@)
    col:position 10000 ;
    a :Row .
\end{lstlisting}
\end{siderules}

  Notice how the \texttt{col:filter} predicate now describes a
  connection to four objects instead of one.

\subsection{Knowledge extracted by \texttt{table2rdf}}

  The \texttt{table2rdf} program extracts all fields in the table.  In addition
  to the knowledge from the table file, \texttt{table2rdf} stores the following
  metadata:

    \begin{table}[H]
      \begin{tabularx}{\textwidth}{*{3}{!{\VRule[-1pt]}l}!{\VRule[-1pt]}L}
      \headrow
      \textbf{Subject} & \textbf{Predicate} & \textbf{Object}
      & \textbf{Description}\\
      \evenrow
      \texttt{:Origin/}\emph{identifier} & \texttt{rdf:type} & \texttt{:Origin}
      & This defines a uniquely identifiable reference to the originating
        file.\\
      \oddrow
      \texttt{:Origin/}\emph{identifier} & \texttt{:filename} &
      \emph{filename}
      & This triple states the originating filename.\\
      \evenrow
      \texttt{:Origin/}\emph{identifier} & \texttt{:convertedBy} &
      \texttt{:table2rdf}
      & This triple states that the file was converted with
        \texttt{table2rdf}.\\
      \oddrow
      \texttt{:Sample/}\emph{sample name} & \texttt{rdf:type} & \texttt{:Sample}
      & This states that there is a sample with \emph{sample name}.\\
      \evenrow
      \texttt{:Sample/}\emph{sample name} & \texttt{:foundIn}
      & \texttt{:Origin/}\emph{identifier}
      & This triple states that a sample can be found in a file identified by
      the \texttt{:Origin} with a specific identifier.\\
    \end{tabularx}
    \caption{\small The additional triple patterns provided by \texttt{table2rdf}.}
    \label{table:table2rdf-ontology}
  \end{table}

  The following snippet is an example of the extra data in Turtle-format:

\begin{siderules}
\begin{verbatim}
<http://sparqling-genomics/table2rdf/1jka8923i4>
    :convertedBy :table2rdf ;
    :filename "grch37.bed"^^xsd:string ;
    a :Origin .

sample:grch37
    :foundIn <http://sparqling-genomics/table2rdf/1jka8923i4> ;
    a :Sample .
\end{verbatim}
\end{siderules}

\subsection{Example usage}

\begin{siderules}
\begin{verbatim}
table2rdf -i /path/to/my/table.tsv > /path/to/my/table.ttl
\end{verbatim}
\end{siderules}

\section{Converting MySQL data to RDF with \texttt{table2rdf}}

  Relational databases store data in tables.  With \texttt{table2rdf} we
  can oftentimes convert the data in a single go to RDF triples.  The following
  example extracts the \texttt{regions} table from a MySQL server in a database
  called \texttt{example}.

\begin{siderules}
\begin{verbatim}
mysql --host=127.0.0.1 -e "SELECT * FROM example.regions" \
      --batch | table2rdf --stdin -O ntriples > regions.n3
\end{verbatim}
\end{siderules}

  The \texttt{mysql} command outputs the table in tab-delimited form when using
  the \texttt{-{}-batch} argument, which is the default input type for
  \texttt{table2rdf}.  To accept input from a UNIX pipe \texttt{table2rdf} must
  be invoked with the \texttt{-{}-stdin} argument.

%% \section{Preparing sequence data with \texttt{fasta2rdf}}
%% \label{sec:fasta2rdf}

%%   Resources like pre-composed reference genomes are often distributed in the
%%   FASTA file format.  The \texttt{fasta2rdf} program generates RDF that
%%   describes each nucleotide, its position (where the first nucleotide is at
%%   position 1, not 0), and to which sequence the nucleotide belongs.

%%   Its main aim is to describe a sequence to allow for querying the sequence
%%   context of a variant.

%% \subsection{Knowledge extracted by \texttt{fasta2rdf}}

%%   The \texttt{fasta2rdf} program extracts a nucleotide and describes it along
%%   with its position in the sequence.

%%   In addition to the knowledge from the FASTA file, \texttt{fasta2rdf} stores the
%%   following metadata:

%%   \begin{table}[H]
%%     \begin{tabularx}{\textwidth}{ l l l L }
%%       \headrow
%%       \textbf{Subject} & \textbf{Predicate} & \textbf{Object}
%%       & \textbf{Description}\\
%%       \evenrow
%%       :Origin & rdf:type & owl:Class
%%       & \texttt{:Origin} is used to identify a data origin (which
%%       is usually a file).\\
%%       \oddrow
%%       :Sample & rdf:type & owl:Class
%%       & \texttt{:Sample} is used to identify a sample name.\\
%%       \evenrow
%%       :Sequence & rdf:type & owl:Class
%%       & \texttt{:Sequence} is used to identify a sequence within the file.
%%       This is typically a chromosome or contig\\
%%       \oddrow
%%       :filename & rdf:type & xsd:string
%%       & \texttt{:filename} contains the path to the file that \texttt{:Origin}
%%       represents.\\
%%       \evenrow
%%       :convertedBy & rdf:type & owl:AnnotationProperty
%%       & \texttt{:convertedBy} is used to identify the program that performed
%%       the VCF->RDF conversion.\\
%%       \oddrow
%%       :foundIn & rdf:type & owl:AnnotationProperty
%%       & \texttt{:foundIn} relates the \texttt{:Origin} to a \texttt{:Sample}.\\
%%     \end{tabularx}
%%     \caption{\small The additional triple patterns described by \texttt{fasta2rdf}.}
%%     \label{table:fasta2rdf-ontology}
%%   \end{table}

%%   The following snippet is an example of the extra data in Turtle-format:

%%   \begin{siderules}
%% \begin{verbatim}
%% <http://sparqling-genomics/fasta2rdf/14f2b609b>
%%     :convertedBy :vcf2rdf ;
%%     :filename "grch37.fasta.gz"^^xsd:string ;
%%     a :Origin .

%% sample:grch37
%%     :foundIn <http://sparqling-genomics/fasta2rdf/14f2b609b3> ;
%%     a :Sample .

%% sample:CLONE_TUMOR
%%     :foundIn <http://sparqling-genomics/fasta2rdf/14f2b609b3> ;
%%     a :Sample .
%% \end{verbatim}
%% \end{siderules}

\section{Extracting knowledge from folders with \texttt{folder2rdf}}
\label{sec:folder2rdf}

  The \texttt{folder2rdf} program finds files in a directory to extract
  knowledge from.  It attempts to convert files with extensions
  \texttt{.vcf}, \texttt{.vcf.gz}, \texttt{.bcf}, and \texttt{.bcf.gz}
  using \texttt{vcf2rdf}, and files with extensions \texttt{.sam},
  \texttt{.bam}, and \texttt{.cram} using \texttt{bam2rdf}.

\subsection{Example usage}

\begin{siderules}
\begin{verbatim}
folder2rdf --input-directory=/vcf-data   \
           --output-directory=/rdf-data  \
           --project-name Example        \
           --recursive                   \
           --compress                    \
           --threads=4
\end{verbatim}
\end{siderules}

  $\ldots{}$ where \texttt{/vcf-data} is a directory containing VCF files,
  and \texttt{/rdf-data} is the directory to store the converted files.

\subsection{Knowledge extracted by \texttt{folder2rdf}}

  In addition to the knowledge extracted by \texttt{vcf2rdf}, this program
  extracts the following data:

  \begin{table}[H]
    \begin{tabularx}{\textwidth}{*{3}{!{\VRule[-1pt]}l}!{\VRule[-1pt]}L}
      \headrow
      \textbf{Subject} & \textbf{Predicate} & \textbf{Object}
      & \textbf{Description}\\
      \evenrow
      \texttt{:Project/}\emph{project-name} & \texttt{rdf:type} & \texttt{:Project}
      & This defines the identifier for the project.\\
      \oddrow
      \texttt{:User/}\emph{username} & \texttt{rdf:type} & \texttt{:User}
      & This defines the identifier for the file owner (username).\\
      \evenrow
      \texttt{:Origin/}\emph{identifier} & \texttt{rdf:type} & \texttt{:Origin}
      & This defines a uniquely identifiable reference to the originating file.\\
    \end{tabularx}
    \caption{\small The additional triple patterns produced by \texttt{folder2rdf}.}
    \label{table:folder2rdf-ontology}
  \end{table}

\section{Importing data with \texttt{curl}}
\label{sec:curl}

  To load RDF data into a triple store (our database), we can use \texttt{curl}.

  The triple stores typically store data in \emph{graphs}.  One triple store
  can host multiple graphs, so we must tell the triple store which graph we
  would like to add the data to.

\subsection{Example usage}

% Other types: application/n-triples
\begin{siderules}
\begin{verbatim}
curl -X POST                                                 \
     -H "Content-Type: text/turtle"                          \
     -T /path/to/variants.ttl                                \
     -G <endpoint URL>                                       \
     --digest -u <username>:<password>                       \
     --data-urlencode graph=http://example/graph
\end{verbatim}
\end{siderules}

\subsubsection{Virtuoso example}

\begin{sloppypar}
The following example inserts the file \texttt{vcf2rdf-variants.ttl} into
a graph called \texttt{http://example/graph} in a Virtuoso endpoint at
\url{http://127.0.0.1:8890} with the username \texttt{dba} and
password \texttt{qwerty}.
\end{sloppypar}

\begin{siderules}
\begin{verbatim}
curl -X POST                                                 \
     -H "Content-Type: text/turtle"                          \
     -T vcf2rdf-variants.ttl                                 \
     -G http://127.0.0.1:8890/sparql-graph-crud-auth         \
     --digest -u dba:qwerty                                  \
     --data-urlencode graph=http://example/graph
\end{verbatim}
\end{siderules}

\subsubsection{4store example}

Similar to the Virtuoso example, for \texttt{4store} the command looks like
this:

\begin{siderules}
\begin{verbatim}
curl -X POST                                                 \
     -H "Content-Type: text/turtle"                          \
     -T vcf2rdf-variants.ttl                                 \
     -G http://127.0.0.1:8080/data/http://example/graph
\end{verbatim}
\end{siderules}

Notice that \texttt{4store} does not provide an authentication mechanism.

\subsubsection{Sending \texttt{gzip}-compressed data}

  When the RDF file is compressed with \texttt{gzip}, extra HTTP headers must
  be added to the \texttt{curl} command:
\begin{siderules}
\begin{verbatim}
curl -X POST                                                 \
     -H "Content-Type: text/turtle"                          \
     -H "Transfer-Encoding: chunked"                         \
     -H "Content-Encoding: gzip"                             \
     ...
\end{verbatim}
\end{siderules}
