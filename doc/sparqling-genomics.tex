\documentclass[11pt,a4paper,oneside]{book}
\usepackage{umcu}
\title{SPARQLing structural variation}
\author{Roel Janssen}

\begin{document}

\begin{titlepage}
  \topskip0pt
  \vspace*{\fill}
  \begin{center}
    \rule{\textwidth}{1.0pt}~\\~\\
    \Huge SPARQLing structural variation
    \rule{\textwidth}{1.0pt}
    \Large Roel Janssen~\\~\\
    \large September 2017
    % Put it a little bit above the center of the page.
    ~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\
  \end{center}
  \topskip0pt
  \vspace*{\fill}

  \thispagestyle{empty}
\end{titlepage}

\setcounter{page}{1}
\pagenumbering{roman}
\hypersetup{linkcolor=black}
\tableofcontents
\newpage{}
\hypersetup{linkcolor=LinkGray}
\setcounter{page}{1}
\pagenumbering{arabic}

\input{introduction}

\chapter{Command-line programs}

  The project provides programs to create a complete pipeline including
  data conversion, data importing and data exploration.  The tasks we can
  perform with the command-line programs are:
  \begin{itemize}
    \item Extract triples from VCF files;
    \item Push data to a SPARQL endpoint.
  \end{itemize}

\section{Preparing data with \texttt{vcf2rdf}}
\label{sec:vcf2rdf}

  Obtaining variants from sequenced data is a task of so called
  \emph{variant callers}.  These programs often output the variants they found
  in the \emph{Variant Call Format} (VCF).  Before we can use the data described
  in this format, we need to extract \emph{knowledge} (in the form of triples)
  from it.

  The \texttt{vcf2rdf} program does exactly this, by converting a VCF file
  into an RDF format.  In section \ref{sec:curl} {\color{LinkGray}`\nameref{sec:curl}%
  '} we describe how to import the data produced by \texttt{vcf2rdf} in the
  database.

\subsection{Knowledge extracted by \texttt{vcf2rdf}}

  The program treats the VCF as its own ontology.  It uses the VCF header as
  a guide.  All fields described in the header of the VCF file will be
  translated into triples.

  In addition to the knowledge from the VCF file, \texttt{vcf2rdf} stores the
  following metadata:

    \begin{table}[H]
    \begin{tabularx}{\textwidth}{ l l l L }
      \headrow
      \textbf{Subject} & \textbf{Predicate} & \textbf{Object}
      & \textbf{Description}\\
      \evenrow
      :Origin & rdf:type & owl:Class
      & \texttt{:Origin} is used to identify a data origin (which
      is usually a file).\\
      \oddrow
      :Sample & rdf:type & owl:Class
      & \texttt{:Sample} is used to identify a sample name.\\
      \evenrow
      :filename & rdf:type & xsd:string
      & \texttt{:filename} contains the path to the file that \texttt{:Origin}
      represents.\\
      \oddrow
      :convertedBy & rdf:type & owl:AnnotationProperty
      & \texttt{:convertedBy} is used to identify the program that performed
      the VCF->RDF conversion.\\
      \evenrow
      :foundIn & rdf:type & owl:AnnotationProperty
      & \texttt{:foundIn} relates the \texttt{:Origin} to a \texttt{:Sample}.\\
    \end{tabularx}
    \caption{\small The additional triple patterns described by \texttt{vcf2rdf}.}
    \label{table:vcf2rdf-ontology}
  \end{table}

  The following snippet is an example of the extra data in Turtle-format:

  \begin{siderules}
\begin{verbatim}
<http://rdf.umcutrecht.nl/vcf2rdf/14f2b609b>
    :convertedBy :vcf2rdf ;
    :filename "clone_ref_tumor.vcf.gz"^^xsd:string ;
    a :Origin .

sample:CLONE_REF
    :foundIn <http://rdf.umcutrecht.nl/vcf2rdf/14f2b609b3> ;
    a :Sample .

sample:CLONE_TUMOR
    :foundIn <http://rdf.umcutrecht.nl/vcf2rdf/14f2b609b3> ;
    a :Sample .
\end{verbatim}
\end{siderules}

\subsection{Example usage}

\begin{siderules}
\begin{verbatim}
vcf2rdf -i /path/to/my/variants.vcf > /path/to/my/variants.ttl
\end{verbatim}
\end{siderules}

\subsection{Run-time properties}

  Depending on the serialization format, the program typically uses from ten megabytes
  (in \texttt{ntriples} mode), to multiple times the size of the input VCF
  (in \texttt{turtle} mode).

  The \texttt{ntriples} mode can output triples as soon as they are formed, while the
  \texttt{turtle} mode waits until all triples are known, so that it can output them
  efficiently, producing compact output at the cost of using more memory.

  We recommend using the \texttt{ntriples} format for large input files, and
  \texttt{turtle} for small input files.

\section{Importing data with \texttt{curl}}
\label{sec:curl}

  To load RDF data into a triple store (our database), we can use \texttt{curl}.

  The triple stores typically store data in \emph{graphs}.  One triple store
  can host multiple graphs, so we must tell the triple store which graph we
  would like to add the data to.

\subsection{Example usage}

% Other types: application/n-triples
\begin{siderules}
\begin{verbatim}
curl -X POST                                                 \
     -H Content-Type:text/turtle                             \
     -T /path/to/variants.ttl                                \
     -G <endpoint URL>                                       \
     --digest -u <username>:<password>                       \
     --data-urlencode graph=http://example/graph
\end{verbatim}
\end{siderules}

\chapter{Web interface}
\label{chap:web-interface}

  In addition to the command-line programs, the project provides a web
  interface for prototyping queries, and quick data reporting.  With the
  web interface you can:
  \begin{itemize}
  \item Write and execute SPARQL queries;
  \item Import and export data;
  \item Visualize triple patterns;
  \item View database statistics.
  \end{itemize}

\section{Running the web interface}

  The web interface can be started using the \texttt{sg-web} command:

\begin{siderules}
\begin{verbatim}
sg-web
\end{verbatim}
\end{siderules}

  By default, it will be available on \url{http://localhost:5000}.

\subsection{Authentication}

  When providing a username and password for a connection, it will attempt
  to connect using \emph{digest authentication}.

\chapter{Information retrieval with SPARQL}

  In section \ref{sec:vcf2rdf} {\color{LinkGray}`%
  \nameref{sec:vcf2rdf}'} we discussed how to extract triples from common data
  formats.  In section \ref{sec:curl} {\color{LinkGray}`\nameref{sec:curl}'} we
  discussed how we could insert those triples into a SPARQL endpoint.

  In this section, we will start exploring the inserted data by using a
  query language called \emph{SPARQL}.  Understanding SPARQL will be crucial
  for the integration in your own programs or scripts --- something we will
  discuss in chapter \ref{chap:programming} {\color{LinkGray}%
  `\nameref{chap:programming}'}.

  The queries in the remainder of this chapter can be readily copy/pasted into
  the query editor of the web interface (see chapter \ref{chap:web-interface}
  {\color{LinkGray}`\nameref{chap:web-interface}'}).

\section{Local querying}

  The promise from ``linked data'' is to make data available in such a way that
  one query can retrieve information from multiple SPARQL endpoints.  We call
  querying over multiple SPARQL endpoints \emph{federated querying}.  But before
  we do that, let's look at simple queries that only look at our own data.

\subsection{Listing non-empty graphs}

  Each SPARQL endpoint can host multiple \emph{graphs}.  Each graph can contain
  an independent set of triples.  The following query displays the available
  graphs in a SPARQL endpoint:

\begin{siderules}
\begin{verbatim}
SELECT DISTINCT ?graph WHERE { GRAPH ?graph { ?s ?p ?o } }
\end{verbatim}
\end{siderules}

\subsection{Querying a specific graph}

  The sooner we can reduce the dataset to query over, the faster the query will
  return with an answer.  One easy way to reduce the size of the dataset is to
  be specific about which graph to query.  This can be achieved using the
  \texttt{FROM} clause in the query.

\begin{siderules}
\begin{verbatim}
SELECT ?s ?p ?o
FROM <graph-name>
WHERE { ?s ?p ?o }
\end{verbatim}
\end{siderules}

  Without the \texttt{FROM} clause, the RDF store will search in all graphs.
  We can repeat the \texttt{FROM} clause to query over multiple graphs in the
  same RDF store.

\begin{siderules}
\begin{verbatim}
SELECT ?s ?p ?o
FROM <graph-name>
FROM <another-graph-name>
WHERE { ?s ?p ?o }
\end{verbatim}
\end{siderules}

  In \ref{sec:federated-querying} {\color{LinkGray}%
  `\nameref{sec:federated-querying}'} we will look at querying over multiple
  RDF stores.

\subsection{Exploring the structure of knowledge in a graph}

  Inside the \texttt{WHERE} clause of a SPARQL query we specify a graph
  pattern.  One useful method of finding out which patterns exist in a
  graph is to look for predicates:

\begin{siderules}
\begin{verbatim}
SELECT DISTINCT ?predicate
FROM <graph-name>
WHERE {
  ?subject ?predicate ?object .
}
\end{verbatim}
\end{siderules}

\subsection{Listing samples and their originating files}

Using the knowledge we gained from exploring the predicates in a graph,
we can construct more insightful queries, like finding the names of the
samples and their originating filenames from the output of \texttt{vcf2rdf}:

\begin{siderules}
\begin{verbatim}
PREFIX vcf2rdf: <http://rdf.umcutrecht.nl/vcf2rdf/>

SELECT DISTINCT STRAFTER(STR(?sample), "Sample/") AS ?sample ?filename
FROM <http://localhost/test>
WHERE {
  ?variant  vcf2rdf:sample    ?sample .
  ?sample   vcf2rdf:foundIn   ?origin .
  ?origin   vcf2rdf:filename  ?filename .
}
\end{verbatim}
\end{siderules}

\subsection{Listing samples, originated files, and number of variants}

Building on the previous query, and by exploring the predicates of a
\texttt{vcf2rdf:VariantCall}, we can construct the following query to
include the number of variants for each sample, in each file.

\begin{siderules}
\begin{verbatim}
PREFIX vcf2rdf: <http://rdf.umcutrecht.nl/vcf2rdf/>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>

SELECT DISTINCT STRAFTER(STR(?sample), "Sample/") AS ?sample
       ?filename
       COUNT(DISTINCT ?variant) AS ?numberOfVariants

FROM <http://localhost/test>
WHERE
{
  ?variant  rdf:type                vcf2rdf:VariantCall ;
            vcf2rdf:sample          ?sample ;
            vcf2rdf:originatedFrom  ?origin .

  ?origin   vcf2rdf:filename        ?filename .
}
\end{verbatim}
\end{siderules}

  By using \texttt{COUNT}, we can get the \texttt{DISTINCT} number of
  matching patterns for a variant call for a sample, originating from
  a distinct file.

\subsection{Retrieving all variants}

  When retrieving potentially large amounts of data, the \texttt{LIMIT}
  clause may come in handy to prototype a query until we are sure enough
  that the query answers the actual question we would like to answer.

  In the next example query, we will retrieve the sample name,
  chromosome, position, and the corresponding VCF \texttt{FILTER} field(s)
  from the database.

\begin{siderules}
\begin{verbatim}
PREFIX vcf2rdf: <http://rdf.umcutrecht.nl/vcf2rdf/>
PREFIX vc: <http://rdf.umcutrecht.nl/vcf2rdf/VariantCall/>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX faldo: <http://biohackathon.org/resource/faldo#>

SELECT DISTINCT ?variant ?sample ?chromosome ?position ?filter
FROM <http://localhost/test>
WHERE
{
  ?variant  rdf:type                vcf2rdf:VariantCall ;
            vcf2rdf:sample          ?sample ;
            faldo:reference         ?chromosome ;
            faldo:position          ?position ;
            vc:FILTER               ?filter .
}
LIMIT 100
\end{verbatim}
\end{siderules}

  By limiting the result set to the first 100 rows, the query will return
  with an answer rather quickly.  Had we not set a limit to the number of
  rows, the query could have returned possibly a few million rows, which
  would obviously take longer to process.

\section{Federated querying}
\label{sec:federated-querying}

  Now that we've seen local queries, there's only one more construct we need to
  know to combine this with remote SPARQL endpoints: the \texttt{SERVICE}
  construct.

  For the next example, we will use the \href{http://www.ebi.ac.uk/rdf/services/sparql}%
  {public SPARQL endpoint hosted by EBI}.

\subsection{Get an overview of Biomodels (from ENSEMBL)}
\begin{siderules}
\begin{verbatim}
PREFIX sbmlrdf: <http://identifiers.org/biomodels.vocabulary#>
PREFIX sbmldb: <http://identifiers.org/biomodels.db/>

SELECT ?speciesId ?name {
  SERVICE <http://www.ebi.ac.uk/rdf/services/sparql/> {
    sbmldb:BIOMD0000000001 sbmlrdf:species ?speciesId .
    ?speciesId sbmlrdf:name ?name
  }
}
\end{verbatim}
\end{siderules}

\chapter{Programming in Python, Perl, R, Scheme, C, and/or C++}
\label{chap:programming}

\section{Using SPARQL with R}
\label{sec:sparql-with-r}

  Before we can start, we need to install the \texttt{SPARQL} package from
  \href{https://cran.r-project.org/web/packages/SPARQL/index.html}{CRAN}.

\begin{siderules}
\begin{verbatim}
install.packages('SPARQL')
\end{verbatim}
\end{siderules}

  Once we're set up, we can query like so:

\begin{siderules}
\begin{verbatim}
# Load the library
library('SPARQL')

# Define the endpoint to query.
endpoint <- "http://localhost:8890/sparql"

# Define the actual query to run.
query <- "PREFIX : <http://localhost:5000/cth/>
PREFIX ref: <http://rdf.ebi.ac.uk/resource/ensembl/90/homo_sapiens/GRCh38/>
PREFIX faldo: <http://biohackathon.org/resource/faldo#>
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT DISTINCT ?variant ?svtype ?geneName ?ensemblGeneId ?location {
    ?variant       rdf:type            :StructuralVariant ;
                   :genome_position    ?p ;
                   :filter             ?filter ;
                   :type               ?svtype .
    ?p             :chromosome         ?chromosome ;
                   :position           ?position .
    ?location      faldo:reference     ref:1 ;
                   faldo:begin         ?begin ;
                   faldo:end           ?end .
    ?begin         faldo:position      ?beginPosition .
    ?end           faldo:position      ?endPosition .
    ?ensemblGeneId rdf:type            ?type ;
                   faldo:location      ?location ;
                   rdfs:label          ?geneName .
    FILTER (?chromosome = \"1\")
    FILTER (?position > (?beginPosition) && ?position < (?endPosition))
}
LIMIT 10";

# Run the query
query_data <- SPARQL (endpoint, query)

# Put the results (a data frame) in a separate variable.
query_results <- query_data$results
\end{verbatim}
\end{siderules}

%\input{performance-tuning}

\end{document}
