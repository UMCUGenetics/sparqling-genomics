\documentclass[11pt,a4paper,oneside]{book}
\usepackage{umcu}
\title{SPARQLing genomics}
\author{Roel Janssen}

\begin{document}

\begin{titlepage}
  \topskip0pt
  \vspace*{\fill}
  \begin{center}
    \includegraphics[width=0.75\textwidth]{figures/logo.pdf}
    %\Huge SPARQLing genomics
    \rule{0.75\textwidth}{1.0pt}~\\
    %\Large Roel Janssen~\\~\\
    \url{https://github.com/UMCUGenetics/sparqling-genomics}~\\
    \large v0.99.4, \today{}
    % Put it a little bit above the center of the page.
    ~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\~\\
  \end{center}
  \topskip0pt
  \vspace*{\fill}

  \thispagestyle{empty}
\end{titlepage}

\setcounter{page}{1}
\pagenumbering{roman}
\hypersetup{linkcolor=black}
\tableofcontents
\newpage{}
\hypersetup{linkcolor=LinkGray}
\setcounter{page}{1}
\pagenumbering{arabic}

\input{introduction}

\chapter{Command-line programs}

  The project provides programs to create a complete pipeline including
  data conversion, data importing and data exploration.  The tasks we can
  perform with the command-line programs are:
  \begin{itemize}
    \item Extract triples from VCF files;
    \item Extract triples from tabular data files;
    \item Push data to a SPARQL endpoint.
  \end{itemize}

\section{Preparing variant call data with \texttt{vcf2rdf}}
\label{sec:vcf2rdf}

  Obtaining variants from sequenced data is a task of so called
  \emph{variant callers}.  These programs often output the variants they found
  in the \emph{Variant Call Format} (VCF).  Before we can use the data described
  in this format, we need to extract \emph{knowledge} (in the form of triples)
  from it.

  The \texttt{vcf2rdf} program does exactly this, by converting a VCF file
  into an RDF format.  In section \ref{sec:curl} {\color{LinkGray}`\nameref{sec:curl}%
  '} we describe how to import the data produced by \texttt{vcf2rdf} in the
  database.

\subsection{Knowledge extracted by \texttt{vcf2rdf}}

  The program treats the VCF as its own ontology.  It uses the VCF header as
  a guide.  All fields described in the header of the VCF file will be
  translated into triples.

  In addition to the knowledge from the VCF file, \texttt{vcf2rdf} stores the
  following metadata:

    \begin{table}[H]
    \begin{tabularx}{\textwidth}{ l l l L }
      \headrow
      \textbf{Subject} & \textbf{Predicate} & \textbf{Object}
      & \textbf{Description}\\
      \evenrow
      :Origin & rdf:type & owl:Class
      & \texttt{:Origin} is used to identify a data origin (which
      is usually a file).\\
      \oddrow
      :Sample & rdf:type & owl:Class
      & \texttt{:Sample} is used to identify a sample name.\\
      \evenrow
      :filename & rdf:type & xsd:string
      & \texttt{:filename} contains the path to the file that \texttt{:Origin}
      represents.\\
      \oddrow
      :convertedBy & rdf:type & owl:AnnotationProperty
      & \texttt{:convertedBy} is used to identify the program that performed
      the VCF->RDF conversion.\\
      \evenrow
      :foundIn & rdf:type & owl:AnnotationProperty
      & \texttt{:foundIn} relates the \texttt{:Origin} to a \texttt{:Sample}.\\
    \end{tabularx}
    \caption{\small The additional triple patterns described by \texttt{vcf2rdf}.}
    \label{table:vcf2rdf-ontology}
  \end{table}

  The following snippet is an example of the extra data in Turtle-format:

\begin{siderules}
\begin{verbatim}
<http://rdf.umcutrecht.nl/vcf2rdf/14f2b609b>
    :convertedBy :vcf2rdf ;
    :filename "clone_ref_tumor.vcf.gz"^^xsd:string ;
    a :Origin .

sample:CLONE_REF
    :foundIn <http://rdf.umcutrecht.nl/vcf2rdf/14f2b609b3> ;
    a :Sample .

sample:CLONE_TUMOR
    :foundIn <http://rdf.umcutrecht.nl/vcf2rdf/14f2b609b3> ;
    a :Sample .
\end{verbatim}
\end{siderules}

\subsection{Example usage}

\begin{siderules}
\begin{verbatim}
vcf2rdf -i /path/to/my/variants.vcf > /path/to/my/variants.ttl
\end{verbatim}
\end{siderules}

\subsection{Run-time properties}

  Depending on the serialization format, the program typically uses from two megabytes
  (in \texttt{ntriples} mode), to multiple times the size of the input VCF
  (in \texttt{turtle} mode).

  The \texttt{ntriples} mode can output triples as soon as they are formed, while the
  \texttt{turtle} mode waits until all triples are known, so that it can output them
  efficiently, producing compact output at the cost of using more memory.

  We recommend using the \texttt{ntriples} format for large input files, and
  \texttt{turtle} for small input files.

\section{Preparing tabular data with \texttt{table2rdf}}
\label{sec:table2rdf}

  Data that can be represented as a table, like comma-separated values (CSV)
  or BED files, can be imported using \texttt{table2rdf}.  The column headers
  are used as predicates, and each row gets a unique row ID.  Non-alphanumeric
  characters in the header line are replaced by underscores, and all characters
  are replaced by their lowercase equivalent to make a consistent scheme for
  predicates.

  When the file does not contain a header line, one can be specified using the
  \texttt{-{}-header-line} argument.  When using this command-line argument, the
  delimiter must be a semicolon (\texttt{;}).

\subsection{Knowledge extracted by \texttt{table2rdf}}

  The \texttt{table2rdf} program extracts all fields in the table.  In addition
  to the knowledge from the table file, \texttt{table2rdf} stores the following
  metadata:

  \begin{table}[H]
    \begin{tabularx}{\textwidth}{ l l l L }
      \headrow
      \textbf{Subject} & \textbf{Predicate} & \textbf{Object}
      & \textbf{Description}\\
      \evenrow
      :Origin & rdf:type & owl:Class
      & \texttt{:Origin} is used to identify a data origin (which
      is usually a file).\\
      \oddrow
      :Sample & rdf:type & owl:Class
      & \texttt{:Sample} is used to identify a sample name.\\
      \evenrow
      :filename & rdf:type & xsd:string
      & \texttt{:filename} contains the path to the file that \texttt{:Origin}
      represents.\\
      \oddrow
      :convertedBy & rdf:type & owl:AnnotationProperty
      & \texttt{:convertedBy} is used to identify the program that performed
      the VCF->RDF conversion.\\
      \evenrow
      :foundIn & rdf:type & owl:AnnotationProperty
      & \texttt{:foundIn} relates the \texttt{:Origin} to a \texttt{:Sample}.\\
    \end{tabularx}
    \caption{\small The additional triple patterns described by \texttt{table2rdf}.}
    \label{table:table2rdf-ontology}
  \end{table}

  The following snippet is an example of the extra data in Turtle-format:

  \begin{siderules}
\begin{verbatim}
<http://rdf.umcutrecht.nl/table2rdf/1jka8923i4>
    :convertedBy :table2rdf ;
    :filename "grch37.bed"^^xsd:string ;
    a :Origin .

sample:grch37
    :foundIn <http://rdf.umcutrecht.nl/table2rdf/1jka8923i4> ;
    a :Sample .
\end{verbatim}
\end{siderules}

\subsection{Example usage}

\begin{siderules}
\begin{verbatim}
table2rdf -i /path/to/my/table.tsv > /path/to/my/table.ttl
\end{verbatim}
\end{siderules}

\section{Converting MySQL data to RDF with \texttt{table2rdf}}

  Relational databases store data in tables.  With \texttt{table2rdf} we
  can oftentimes convert the data in a single go to RDF triples.  The following
  example extracts the \texttt{regions} table from a MySQL server in a database
  called \texttt{example}.

\begin{siderules}
\begin{verbatim}
mysql --host=127.0.0.1 -e "SELECT * FROM example.regions" \
      --batch | table2rdf --stdin -O ntriples > regions.n3
\end{verbatim}
\end{siderules}

  The \texttt{mysql} command outputs the table in tab-delimited form when using
  the \texttt{-{}-batch} argument, which is the default input type for
  \texttt{table2rdf}.  To accept input from a UNIX pipe \texttt{table2rdf} must
  be invoked with the \texttt{-{}-stdin} argument.

%% \section{Preparing sequence data with \texttt{fasta2rdf}}
%% \label{sec:fasta2rdf}

%%   Resources like pre-composed reference genomes are often distributed in the
%%   FASTA file format.  The \texttt{fasta2rdf} program generates RDF that
%%   describes each nucleotide, its position (where the first nucleotide is at
%%   position 1, not 0), and to which sequence the nucleotide belongs.

%%   Its main aim is to describe a sequence to allow for querying the sequence
%%   context of a variant.

%% \subsection{Knowledge extracted by \texttt{fasta2rdf}}

%%   The \texttt{fasta2rdf} program extracts a nucleotide and describes it along
%%   with its position in the sequence.

%%   In addition to the knowledge from the FASTA file, \texttt{fasta2rdf} stores the
%%   following metadata:

%%   \begin{table}[H]
%%     \begin{tabularx}{\textwidth}{ l l l L }
%%       \headrow
%%       \textbf{Subject} & \textbf{Predicate} & \textbf{Object}
%%       & \textbf{Description}\\
%%       \evenrow
%%       :Origin & rdf:type & owl:Class
%%       & \texttt{:Origin} is used to identify a data origin (which
%%       is usually a file).\\
%%       \oddrow
%%       :Sample & rdf:type & owl:Class
%%       & \texttt{:Sample} is used to identify a sample name.\\
%%       \evenrow
%%       :Sequence & rdf:type & owl:Class
%%       & \texttt{:Sequence} is used to identify a sequence within the file.
%%       This is typically a chromosome or contig\\
%%       \oddrow
%%       :filename & rdf:type & xsd:string
%%       & \texttt{:filename} contains the path to the file that \texttt{:Origin}
%%       represents.\\
%%       \evenrow
%%       :convertedBy & rdf:type & owl:AnnotationProperty
%%       & \texttt{:convertedBy} is used to identify the program that performed
%%       the VCF->RDF conversion.\\
%%       \oddrow
%%       :foundIn & rdf:type & owl:AnnotationProperty
%%       & \texttt{:foundIn} relates the \texttt{:Origin} to a \texttt{:Sample}.\\
%%     \end{tabularx}
%%     \caption{\small The additional triple patterns described by \texttt{fasta2rdf}.}
%%     \label{table:fasta2rdf-ontology}
%%   \end{table}

%%   The following snippet is an example of the extra data in Turtle-format:

%%   \begin{siderules}
%% \begin{verbatim}
%% <http://rdf.umcutrecht.nl/fasta2rdf/14f2b609b>
%%     :convertedBy :vcf2rdf ;
%%     :filename "grch37.fasta.gz"^^xsd:string ;
%%     a :Origin .

%% sample:grch37
%%     :foundIn <http://rdf.umcutrecht.nl/fasta2rdf/14f2b609b3> ;
%%     a :Sample .

%% sample:CLONE_TUMOR
%%     :foundIn <http://rdf.umcutrecht.nl/fasta2rdf/14f2b609b3> ;
%%     a :Sample .
%% \end{verbatim}
%% \end{siderules}

\section{Importing data with \texttt{curl}}
\label{sec:curl}

  To load RDF data into a triple store (our database), we can use \texttt{curl}.

  The triple stores typically store data in \emph{graphs}.  One triple store
  can host multiple graphs, so we must tell the triple store which graph we
  would like to add the data to.

\subsection{Example usage}

% Other types: application/n-triples
\begin{siderules}
\begin{verbatim}
curl -X POST                                                 \
     -H Content-Type:text/turtle                             \
     -T /path/to/variants.ttl                                \
     -G <endpoint URL>                                       \
     --digest -u <username>:<password>                       \
     --data-urlencode graph=http://example/graph
\end{verbatim}
\end{siderules}

\subsubsection{Virtuoso example}

\begin{sloppypar}
The following example inserts the file \texttt{vcf2rdf-variants.ttl} into
a graph called \texttt{http://example/graph} in a Virtuoso endpoint at
\url{http://127.0.0.1:8890} with the username \texttt{dba} and
password \texttt{qwerty}.
\end{sloppypar}

\begin{siderules}
\begin{verbatim}
curl -X POST                                                 \
     -H Content-Type:text/turtle                             \
     -T vcf2rdf-variants.ttl                                 \
     -G http://127.0.0.1:8890/sparql-graph-crud-auth         \
     --digest -u dba:qwerty                                  \
     --data-urlencode graph=http://example/graph
\end{verbatim}
\end{siderules}

\subsubsection{4store example}

Similar to the Virtuoso example, for \texttt{4store} the command looks like
this:

\begin{siderules}
\begin{verbatim}
curl -X POST                                                 \
     -H Content-Type:text/turtle                             \
     -T vcf2rdf-variants.ttl                                 \
     -G http://127.0.0.1:8080/data/http://example/graph
\end{verbatim}
\end{siderules}

Notice that \texttt{4store} does not provide an authentication mechanism.

\chapter{Web interface}
\label{chap:web-interface}

  In addition to the command-line programs, the project provides a web
  interface for prototyping queries, and quick data reporting.  With the
  web interface you can:
  \begin{itemize}
  \item Write and execute SPARQL queries;
  \item Keep track of different SPARQL endpoints.
  \end{itemize}

\section{Running the web interface}

  The web interface can be started using the \texttt{sg-web} command:

\begin{siderules}
\begin{verbatim}
sg-web
\end{verbatim}
\end{siderules}

  By default, it will be accessible on \url{http://localhost:5000}.

\subsection{Configurating connections}

  The first useful step is to configure a connection to a SPARQL endpoint.

  \begin{figure}[h]
    \begin{center}
      \includegraphics[width=1.0\textwidth]{figures/web-connections.png}
    \end{center}
    \caption{The \emph{connections} page enables users to configure accessible
      SPARQL endpoints.  Adding a connection here will provide an option to
      query it on the \emph{query} page.}
    \label{fig:web-connections}
  \end{figure}

  When providing a username and password for a connection, it will attempt
  to connect using \emph{digest authentication}.

\subsection{Executing queries}

  After configuring at least one endpoint, it can be chosen on the \emph{query}
  page to execute a query against it.

  \begin{figure}[h]
    \begin{center}
      \includegraphics[width=1.0\textwidth]{figures/web-query.png}
    \end{center}
    \caption{The \emph{query} page enables users to execute a query against a
      SPARQL endpoint.  The connections configured at the \emph{connections} page
      can be chosen from the drop-down menu.}
    \label{fig:web-query}
  \end{figure}

\input{information-retrieval}

\chapter{Using SPARQL with other programming languages}
\label{chap:programming}

\section{Using SPARQL with R}
\label{sec:sparql-with-r}

  Before we can start, we need to install the \texttt{SPARQL} package from
  \href{https://cran.r-project.org/web/packages/SPARQL/index.html}{CRAN}.

\begin{siderules}
\begin{verbatim}
install.packages('SPARQL')
\end{verbatim}
\end{siderules}

  Once the package is installed, we can load it:

\begin{siderules}
\begin{verbatim}
library('SPARQL')
\end{verbatim}
\end{siderules}

  Let's define where to send the query to:
\begin{siderules}
\begin{verbatim}
endpoint <- "http://localhost:8890/sparql"
\end{verbatim}
\end{siderules}

  $\ldots{}$ and the query itself:
\begin{siderules}
\begin{verbatim}
query <- "PREFIX vcf2rdf: <http://rdf.umcutrecht.nl/vcf2rdf/>
PREFIX vc:    <http://rdf.umcutrecht.nl/vcf2rdf/VariantCall/>
PREFIX rdf:   <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX faldo: <http://biohackathon.org/resource/faldo#>

SELECT DISTINCT ?variant ?sample ?chromosome ?position ?filter
FROM <graph-name>
WHERE
{
  ?variant  rdf:type                vcf2rdf:VariantCall ;
            vcf2rdf:sample          ?sample ;
            faldo:reference         ?chromosome ;
            faldo:position          ?position ;
            vc:FILTER               ?filter .
}
LIMIT 10";
\end{verbatim}
\end{siderules}

  To actually execute the query, we can use the \texttt{SPARQL} function:
\begin{siderules}
\begin{verbatim}
query_data <- SPARQL (endpoint, query)
\end{verbatim}
\end{siderules}

  If the query execution went fine, we can gather the resulting
  \texttt{dataframe} from the \texttt{results} index.

\begin{siderules}
\begin{verbatim}
query_results <- query_data$results
\end{verbatim}
\end{siderules}

\subsection{Querying with authentication}

  When the SPARQL endpoint we try to reach requires authentication before
  it accepts a query, we can use the \texttt{curl\_args} parameter of the
  \texttt{SPARQL} function.

  In the following example, we use \texttt{dba} as username, and
  \texttt{secret-password} as password.

\begin{siderules}
\begin{verbatim}
endpoint     <- "http://localhost:8890/sparql-auth"
auth_options <- curlOptions(userpwd="dba:secret-password")
query        <- "SELECT DISTINCT ?p WHERE { ?s ?p ?o }"
query_data   <- SPARQL (endpoint, query, curl_args=auth_options)
results      <- query_data$results
\end{verbatim}
\end{siderules}

\section{Using SPARQL with GNU Guile}
\label{sec:sparql-with-guile}

  For Schemers using GNU Guile, the \href{https://github.com/roelj/guile-sparql}%
  {guile-sparql}\footnote{\url{https://github.com/roelj/guile-sparql}} package
  provides a SPARQL interface.

  The package provides a \texttt{driver} module that communicates with the
  SPARQL endpoint, a \texttt{lang} module to construct SPARQL queries using
  S-expressions, and a \texttt{util} module containing convenience functions.

  After installation, the modules can be loaded using:

\begin{siderules}
\begin{verbatim}
(use-modules (sparql driver)
             (sparql lang)
             (sparql util))
\end{verbatim}
\end{siderules}

  Using the \texttt{sparql-query} function, we can execute a query:

\begin{siderules}
\begin{verbatim}
(let ((endpoint       "http://localhost:8890/sparql-auth")
      (authentication "dba:secret-password")
      (query          "SELECT DISTINCT ?p WHERE { ?s ?p ?o }"))
  (display-query-results-of
    (sparql-query query
                  #:uri    endpoint
                  #:digest authentication)))
\end{verbatim}
\end{siderules}

\end{document}
